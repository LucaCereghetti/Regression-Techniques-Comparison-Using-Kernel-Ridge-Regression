{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7Mdjx40d5La"
   },
   "source": [
    "# **Reproducing Kernel Hilbert Space**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EJKZeNtd-Um"
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Tz9qHcicd_HP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import Markdown\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98HsBb3jeWn6"
   },
   "source": [
    "## Load functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ist7dkjgDOb"
   },
   "source": [
    "Please follows the following steps:\n",
    "1. Initiate by executing the notebook titled 'Kernel_function_def.ipynb'. \\\\\n",
    "2. Upon completion, the notebook will automatically save as 'kernel_functions.py'. \\\\\n",
    "3. Load the kernel evaluation functions into the current notebook by running the command %run kernel_functions.py as shown in code snippet [1]. \\\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "k0DdrUcIOO-o",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the python file containing the kernel functions\n",
    "%run kernel_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPLiFBoxeBja"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4h-CepOTOffO"
   },
   "source": [
    "Consider that you have training inputs/samples $(x_i)_{i=1}^n \\in \\mathbb{R}^d$ and targets $(y_i)_{i=1}^n \\in \\mathbb{R}$, where $d$ is the number of features. We can define the *design matrix* and the *output vector* respectively as\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{X} := \\begin{bmatrix} x_1^\\top \\\\ \\vdots \\\\ x_n^\\top\\end{bmatrix} \\in \\mathbb{R}^{n \\times d}, \\qquad \\textbf{y} := \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n\\end{bmatrix} \\in \\mathbb{R}^{n}.\n",
    "\\end{equation}\n",
    "\n",
    "Now, suppose that we have $(z_i)_{i=1}^m \\in \\mathbb{R}^d$ which are the *test inputs* and we want to predict our models on this set of test data. We define the following\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{Z} := \\begin{bmatrix} z_1^\\top \\\\ \\vdots \\\\ z_m^\\top\\end{bmatrix} \\in \\mathbb{R}^{m \\times d}, \\qquad {\\textbf{y}}^\\text{pred} := \\begin{bmatrix} {f}^\\star(z_1) \\\\ \\vdots \\\\ f^\\star(z_m) \\end{bmatrix} \\in \\mathbb{R}^{m},\n",
    "\\end{equation}\n",
    "\n",
    "where ${\\textbf{y}}^\\text{pred}$ is the vector of *predicted outputs*. Given the actual targets for the test data set, which we denote as\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{y}^{\\text{true}} := \\begin{bmatrix} y_1^\\text{true} \\\\ \\vdots \\\\ y_m^\\text{true} \\end{bmatrix} \\in \\mathbb{R}^{m},\n",
    "\\end{equation}\n",
    "\n",
    "we can denote the vector of *residuals* as\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{ɛ} := {\\textbf{y}}^\\text{pred} - \\textbf{y}^\\text{true}.\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_wm_6t1OkZw"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wzpCA4SFeFI4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame sample:\n",
      "   No  X1 transaction date  X2 house age  \\\n",
      "0   1             2012.917          32.0   \n",
      "1   2             2012.917          19.5   \n",
      "2   3             2013.583          13.3   \n",
      "\n",
      "   X3 distance to the nearest MRT station  X4 number of convenience stores  \\\n",
      "0                                84.87882                               10   \n",
      "1                               306.59470                                9   \n",
      "2                               561.98450                                5   \n",
      "\n",
      "   X5 latitude  X6 longitude  Y house price of unit area  \n",
      "0     24.98298     121.54024                        37.9  \n",
      "1     24.98034     121.53951                        42.2  \n",
      "2     24.98746     121.54391                        47.3  \n"
     ]
    }
   ],
   "source": [
    "# Read the data file\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Extract columns containing the feature vectors\n",
    "cols = [1,2,3,4,5,6]\n",
    "\n",
    "#  Store the input feature vectors in X\n",
    "X = df[df.columns[cols]]\n",
    "\n",
    "# Store the target variables in y\n",
    "y = df[df.columns[[7]]]\n",
    "\n",
    "# Convert to numpy array\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(\"DataFrame sample:\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHINrhbiPR7P"
   },
   "source": [
    "We now split the dataset into a training set and a testing set. We use the following variables:\n",
    "\n",
    "1. X_train : matrix $\\textbf{X}$\n",
    "\n",
    "2. X_test : matrix $\\textbf{Z}$\n",
    "\n",
    "3. y_train : vector $\\textbf{y}$\n",
    "\n",
    "4. y_test : vector $\\textbf{y}^\\text{true}$\n",
    "\n",
    "Before using the models, you should first standardize the training dataset and the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dU_A-ImBY_MV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# Split X and y into X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)\n",
    "\n",
    "# Calculate mean and standard deviation of each column\n",
    "mean = np.mean(X_train, axis = 0)\n",
    "standard_deviation = np.std(X_train, axis = 0)\n",
    "\n",
    "# Standardize X_train and X_test\n",
    "X_train = (X_train - mean)/standard_deviation\n",
    "X_test = (X_test - mean)/standard_deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTr-mVv3P9DO"
   },
   "source": [
    "## Error metrics for model performance comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9hICKCfP90H"
   },
   "source": [
    "For comparison of model performance, we consider six types of errors:\n",
    "\n",
    "\n",
    " 1. $\\ell^1$ norm of $\\textbf{ɛ}$: $\\sum_{i=1}^m |ɛ_i|$.\n",
    "\n",
    "\n",
    " 2. Mean absolute error of $\\textbf{ɛ}$: $\\frac{1}{m}\\sum_{i=1}^m |ɛ _i|$.\n",
    "\n",
    "\n",
    "\n",
    " 3. Relative absolute error of $\\textbf{ɛ}$ : $\\dfrac{\\sum_{i=1}^m |ɛ _i|}{\\sum_{i=1}^m |y_i^\\text{true} - \\overline{y^\\text{true}}|}$.\n",
    "\n",
    "\n",
    "\n",
    " 4. $\\ell^2$ norm of $\\textbf{ɛ}$: $\\left(\\sum_{i=1}^m |ɛ_i|^2\\right)^{1/2}$.\n",
    "\n",
    "\n",
    " 5. Mean squared error of $\\textbf{ɛ}$: $\\frac{1}{m}\\sum_{i=1}^m |ɛ_i|^2$.\n",
    "\n",
    "\n",
    "\n",
    " 6. Relative squared error of $\\textbf{ɛ}$: $\\dfrac{\\sum_{i=1}^m |ɛ _i|^2}{\\sum_{i=1}^m |y_i^\\text{true} - \\overline{y^\\text{true}}|^2}$.\n",
    "\n",
    "\n",
    "In the above formulae, $\\overline{y^\\text{true}} := \\frac{1}{m} \\sum_{i=1}^m y_i^\\text{true}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4EG6egNUP_5j",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def errors(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate various error metrics for model performance comparison.\n",
    "\n",
    "    Parameters:\n",
    "    y_test (iterable): True values.\n",
    "    y_pred (iterable): Predicted values.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the calculated error metrics.\n",
    "    \"\"\"\n",
    "    # Calculate the l_1 norm\n",
    "    error_1 = np.sum(np.abs(y_pred - y_test))\n",
    "    print(np.mean(np.abs(y_pred-y_test)))\n",
    "\n",
    "    # Calculate the mean absolute error\n",
    "    mean_absolute_error = np.mean(np.abs(y_pred - y_test))\n",
    "\n",
    "    # Calculate the relative absolute \n",
    "    y_true_mean = np.mean(y_test) # Calculate the mean of y_true\n",
    "    relative_absolute_error = np.sum(np.abs(y_pred - y_test)) / np.sum(np.abs(y_test - y_true_mean))\n",
    "        \n",
    "    # Calculate the l_2 norm\n",
    "    error_2 = np.sqrt(np.sum((y_pred - y_test)** 2))\n",
    "\n",
    "    # Calculate the mean squared error\n",
    "    mean_squared_error = np.mean((y_pred - y_test) ** 2)\n",
    "\n",
    "    # Calculate the relative squared error\n",
    "    relative_squared_error = np.sum(np.abs(y_pred - y_test) ** 2) / np.sum((y_test - y_true_mean) ** 2)\n",
    "    \n",
    "    return (error_1, mean_absolute_error, relative_absolute_error, error_2, mean_squared_error, relative_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nS4FNC6geIQR"
   },
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O10uWQMOSCz-"
   },
   "source": [
    "The optimal parameter $\\mathbf{\\beta}^\\star$ obtained from\n",
    "\n",
    "\\begin{equation}\n",
    "  \\beta^\\star := \\underset{\\beta \\in \\mathbb{R}^n}{\\operatorname{argmin}} \\frac{1}{n} \\left\\|\\textbf{y} - \\textbf{X}\\beta\\right\\|_2^2\n",
    "\\end{equation}\n",
    "\n",
    "is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta^\\star = (\\textbf{X}^\\top \\textbf{X})^{-1} \\textbf{X}^\\top \\textbf{y}.\n",
    "\\end{equation}\n",
    "\n",
    "We predict at the test inputs $\\textbf{Z}$ by\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{y}^\\text{pred} = \\textbf{Z}\\beta^\\star.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_G5cKGvkeJBj"
   },
   "outputs": [],
   "source": [
    "def linear_regression():\n",
    "    X_train_T = X_train.T\n",
    "    ones_column = np.ones((X_train.shape[0], 1))  \n",
    "    X_train_with_intercept = np.hstack((ones_column, X_train))  \n",
    "    beta_star = np.linalg.inv(X_train_with_intercept.T @ X_train_with_intercept) @ X_train_with_intercept.T @ y_train\n",
    "\n",
    "    # Predict on the test set\n",
    "    ones_column_test = np.ones((X_test.shape[0], 1))  \n",
    "    X_test_with_intercept = np.hstack((ones_column_test, X_test))  \n",
    "    y_pred = X_test_with_intercept @ beta_star\n",
    "\n",
    "    # print(y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vuoucIuBSznl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.274984907782409\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|                         |   Statistics for linear regression |\n",
       "|:------------------------|-----------------------------------:|\n",
       "| $\\ell^1$ Error          |                         784.373    |\n",
       "| Mean Absolute Error     |                           6.27498  |\n",
       "| Relative Absolute Error |                           0.543918 |\n",
       "| $\\ell^2$ Error          |                         114.873    |\n",
       "| Mean Squared Error      |                         105.566    |\n",
       "| Relative Squared Error  |                           0.458135 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict y using linear regression\n",
    "y_pred = linear_regression()\n",
    "\n",
    "# Obtain error metrics\n",
    "error_1, mean_absolute_error, relative_absolute_error, \\\n",
    "error_2, mean_squared_error, relative_squared_error = errors(y_test, y_pred)\n",
    "\n",
    "# Display the error metrics\n",
    "stats = np.array([error_1, mean_absolute_error, relative_absolute_error, error_2, mean_squared_error, relative_squared_error])\n",
    "stats = pd.DataFrame(stats, columns = ['Statistics for linear regression'])\n",
    "stats.index = [r\"$\\ell^1$ Error\", 'Mean Absolute Error', 'Relative Absolute Error', \\\n",
    "               r\"$\\ell^2$ Error\", 'Mean Squared Error', 'Relative Squared Error']\n",
    "display(Markdown(stats.to_markdown()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xBqCTloeJOn"
   },
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBhb_KYySYSj"
   },
   "source": [
    "The optimal parameter $\\mathbf{\\beta}^\\star$ obtained from\n",
    "\n",
    "\\begin{equation}\n",
    "  \\beta^\\star := \\underset{\\beta \\in \\mathbb{R}^n}{\\operatorname{argmin}} \\frac{1}{n} \\left\\|\\textbf{y} - \\textbf{X}\\beta\\right\\|_2^2 + \\mu \\|\\beta\\|_2^2\n",
    "\\end{equation}\n",
    "\n",
    "is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta^\\star = \\left(\\textbf{X}^\\top \\textbf{X} + n \\mu \\textbf{I}\\right)^{-1} \\textbf{X}^\\top \\textbf{y}.\n",
    "\\end{equation}\n",
    "\n",
    "We predict at the test inputs $\\textbf{Z}$ by\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{y}^\\text{pred} = \\textbf{Z}\\beta^\\star.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_with_intercept(mu):\n",
    "    X_train_intercept = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
    "    X_test_intercept = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
    "    \n",
    "    n = X_train_intercept.shape[0]\n",
    "    X_train_T = X_train_intercept.T\n",
    "    I = np.identity(X_train_intercept.shape[1])\n",
    "    I[0, 0] = 0 \n",
    "    \n",
    "    beta_star = np.linalg.inv(X_train_T @ X_train_intercept + n * mu * I) @ X_train_T @ y_train\n",
    "    y_pred = X_test_intercept @ beta_star\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.360906919486817\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|                         |   Statistics for ridge regression |\n",
       "|:------------------------|----------------------------------:|\n",
       "| $\\ell^1$ Error          |                        795.113    |\n",
       "| Mean Absolute Error     |                          6.36091  |\n",
       "| Relative Absolute Error |                          0.551366 |\n",
       "| $\\ell^2$ Error          |                        116.041    |\n",
       "| Mean Squared Error      |                        107.723    |\n",
       "| Relative Squared Error  |                          0.467499 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose a penalty factor below. You can try with mu = 0.01, 0.1, 1, 10, 100\n",
    "mu = 0.1\n",
    "# Predict y using ridge regression\n",
    "y_pred = ridge_regression_with_intercept(mu)\n",
    "\n",
    "# Obtain error metrics\n",
    "error_1, mean_absolute_error, relative_absolute_error, \\\n",
    "error_2, mean_squared_error, relative_squared_error = errors(y_test, y_pred)\n",
    "\n",
    "# Display the error metrics\n",
    "stats = np.array([error_1, mean_absolute_error, relative_absolute_error, error_2, mean_squared_error, relative_squared_error])\n",
    "stats = pd.DataFrame(stats, columns=['Statistics for ridge regression'])\n",
    "stats.index = [r\"$\\ell^1$ Error\", 'Mean Absolute Error', 'Relative Absolute Error', r\"$\\ell^2$ Error\", 'Mean Squared Error', 'Relative Squared Error']\n",
    "display(Markdown(stats.to_markdown()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvjqRGyWeOJO"
   },
   "source": [
    "## Kernel ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gFjND8HUoQA"
   },
   "source": [
    "Using the *Representer theorem*, the optimal solution to the learning problem\n",
    "\n",
    "\\begin{equation}\n",
    "f^\\star(x) := \\underset{f \\in \\mathcal{H}}{\\operatorname{argmin}} \\frac{1}{n} \\sum_{i=1}^n \\Big(y_i - f(x_i)\\Big)^2 + \\mu \\|f\\|_\\mathcal{H}^2\n",
    "\\end{equation}\n",
    "\n",
    "is given by\n",
    "\n",
    "\\begin{equation}\n",
    "f^\\star(x) = \\sum_{i=1}^n \\alpha_i^\\star K(x_i, x),\n",
    "\\end{equation}\n",
    "\n",
    "The optimal coefficients $\\alpha_i^\\star, \\, 1\\leq i \\leq n$ are obtained as\n",
    "\n",
    "\\begin{equation}\n",
    " \\alpha^\\star := \\begin{bmatrix} \\alpha_1^\\star \\\\ \\vdots \\\\ \\alpha_n^\\star \\end{bmatrix} = \\left(\\textbf{K} + n\\mu \\textbf{I}\\right)^{-1} \\textbf{y} \\in \\mathbb{R}^n,\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{K} :=\n",
    "\\begin{bmatrix}\n",
    "K(x_1, x_1) & \\cdots & K(x_1, x_n)\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "K(x_n, x_1) & \\cdots & K(x_n,x_n)\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{n \\times n}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "is the *kernel matrix*.\n",
    "\n",
    "We predict at the test inputs $\\textbf{Z}$ by\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{y}^\\text{pred} = \\textbf{G} \\alpha^\\star,\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{G} :=\n",
    "\\begin{bmatrix}\n",
    "K(z_1, x_1) & \\cdots & K(z_1, x_n) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "K(z_m, x_1) & \\cdots & K(z_m, x_n)\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{m \\times n}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_ridge_regression(X_train, y_train, X_test, kernel_type='convex', mu=0.1, weights=None, length_scales=None, kernel_choice_type=None, kernel_length_scale=1.0):\n",
    "    \n",
    "    n = X_train.shape[0]\n",
    "\n",
    "    if kernel_type == 'convex':\n",
    "        if weights is None or length_scales is None:\n",
    "            raise ValueError(\"Weights and length_scales must be provided for convex kernel.\")\n",
    "        K, G = convex_kernel(X_train, X_test, weights, length_scales)\n",
    "    elif kernel_type == 'product':\n",
    "        if length_scales is None:\n",
    "            raise ValueError(\"Length_scales must be provided for product kernel.\")\n",
    "        K, G = product_kernel(X_train, X_test, length_scales)\n",
    "    elif kernel_type == 'choice':\n",
    "        if kernel_choice_type is None:\n",
    "            raise ValueError(\"kernel_choice_type must be provided for kernel_choice.\")\n",
    "        K, G = kernel_choice(X_train, X_test ,kernel_choice_type, kernel_length_scale)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Invalid kernel type. Choose 'convex', 'product', or 'choice'.\")\n",
    "\n",
    "    # Compute the optimal coefficients alpha_star\n",
    "    I = np.identity(n)\n",
    "    alpha_star = np.linalg.inv(K + n * mu * I) @ y_train\n",
    "\n",
    "    # Predict the values for the test set\n",
    "    y_pred = G @ alpha_star\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.672864395408705\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|                         |   Statistics for kernel ridge regression |\n",
       "|:------------------------|-----------------------------------------:|\n",
       "| $\\ell^1$ Error          |                               2834.11    |\n",
       "| Mean Absolute Error     |                                 22.6729  |\n",
       "| Relative Absolute Error |                                  1.96529 |\n",
       "| $\\ell^2$ Error          |                                288.126   |\n",
       "| Mean Squared Error      |                                664.135   |\n",
       "| Relative Squared Error  |                                  2.88222 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose a penalty factor below. You can try with mu = 0.01, 0.1, 1, 10, 100\n",
    "mu =0.1\n",
    "\n",
    "# Predict y using kernel ridge regression (single kernel)   \n",
    "y_pred = kernel_ridge_regression(X_train, y_train, X_test, mu=mu, kernel_type='choice', kernel_choice_type=\"RBF\")\n",
    "\n",
    "# Obtain error metrics\n",
    "error_1, mean_absolute_error, relative_absolute_error, \\\n",
    "error_2, mean_squared_error, relative_squared_error = errors(y_test, y_pred)\n",
    "\n",
    "# Display the error metrics\n",
    "stats = np.array([error_1, mean_absolute_error, relative_absolute_error, error_2, mean_squared_error, relative_squared_error])\n",
    "stats = pd.DataFrame(stats, columns = ['Statistics for kernel ridge regression'])\n",
    "stats.index = [r\"$\\ell^1$ Error\", 'Mean Absolute Error', 'Relative Absolute Error', \\\n",
    "               r\"$\\ell^2$ Error\", 'Mean Squared Error', 'Relative Squared Error']\n",
    "display(Markdown(stats.to_markdown()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7KpwkeaVK7v"
   },
   "source": [
    "## KRR with different combination of kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XC79s1O1VNZr"
   },
   "source": [
    "We now seek to construct more general kernel functions. For this purpose, refer to the file 'kernel_functions.py' and follow the instructions written there.  Finally, having implemented the kernel function, we want to perform kernel ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3UUHp5aVR3u"
   },
   "source": [
    "## KRR with convex combination of kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Q0R9YPmFVeib",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.34963934599475\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|                         |   Statistics for kernel ridge regression |\n",
       "|:------------------------|-----------------------------------------:|\n",
       "| $\\ell^1$ Error          |                               2793.7     |\n",
       "| Mean Absolute Error     |                                 22.3496  |\n",
       "| Relative Absolute Error |                                  1.93727 |\n",
       "| $\\ell^2$ Error          |                                284.814   |\n",
       "| Mean Squared Error      |                                648.951   |\n",
       "| Relative Squared Error  |                                  2.81632 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose a penalty factor below. You can try with mu = 0.01, 0.1, 1, 10, 100\n",
    "mu = 0.1\n",
    "weights = {'RBF': 0.5, 'Matern': 0.3, 'Laplacian': 0.2}\n",
    "length_scales = {'RBF': 1.0, 'Matern': 1.0, 'Laplacian': 1.0}\n",
    "\n",
    "# Predict y using kernel ridge regression (convex combinations)\n",
    "y_pred = kernel_ridge_regression(X_train, y_train, X_test, kernel_type='convex', mu=mu, weights=weights, length_scales=length_scales)\n",
    "\n",
    "# Obtain error metrics\n",
    "error_1, mean_absolute_error, relative_absolute_error, \\\n",
    "error_2, mean_squared_error, relative_squared_error = errors(y_test, y_pred)\n",
    "\n",
    "# Display the error metrics\n",
    "stats = np.array([error_1, mean_absolute_error, relative_absolute_error, error_2, mean_squared_error, relative_squared_error])\n",
    "stats = pd.DataFrame(stats, columns = ['Statistics for kernel ridge regression'])\n",
    "stats.index = [r\"$\\ell^1$ Error\", 'Mean Absolute Error', 'Relative Absolute Error', \\\n",
    "               r\"$\\ell^2$ Error\", 'Mean Squared Error', 'Relative Squared Error']\n",
    "display(Markdown(stats.to_markdown()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SIVaeDlUdGv"
   },
   "source": [
    "## KRR with product of kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qK_2UOsGVqzf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.785819975601974\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|                         |   Statistics for kernel ridge regression |\n",
       "|:------------------------|-----------------------------------------:|\n",
       "| $\\ell^1$ Error          |                               4348.23    |\n",
       "| Mean Absolute Error     |                                 34.7858  |\n",
       "| Relative Absolute Error |                                  3.01525 |\n",
       "| $\\ell^2$ Error          |                                419.599   |\n",
       "| Mean Squared Error      |                               1408.51    |\n",
       "| Relative Squared Error  |                                  6.11264 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose a penalty factor below. You can try with mu = 0.01, 0.1, 1, 10, 100\n",
    "mu = 0.1\n",
    "length_scales = {'RBF': 1.0, 'Matern': 1.0, 'Laplacian': 1.0}\n",
    "\n",
    "# Predict y using kernel ridge regression (convex combinations)\n",
    "y_pred = kernel_ridge_regression(X_train, y_train, X_test, kernel_type='product', mu=mu, length_scales=length_scales)\n",
    "\n",
    "# Obtain error metrics\n",
    "error_1, mean_absolute_error, relative_absolute_error, \\\n",
    "error_2, mean_squared_error, relative_squared_error = errors(y_test, y_pred)\n",
    "\n",
    "# Display the error metrics\n",
    "stats = np.array([error_1, mean_absolute_error, relative_absolute_error, error_2, mean_squared_error, relative_squared_error])\n",
    "stats = pd.DataFrame(stats, columns = ['Statistics for kernel ridge regression'])\n",
    "stats.index = [r\"$\\ell^1$ Error\", 'Mean Absolute Error', 'Relative Absolute Error', \\\n",
    "               r\"$\\ell^2$ Error\", 'Mean Squared Error', 'Relative Squared Error']\n",
    "display(Markdown(stats.to_markdown()))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
